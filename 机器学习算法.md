## 机器学习算法 -note

1. 线性回归

2. 逻辑回归

3. 决策树,随机森林， GBDT

   * 1） 开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。

     2） 如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。

     3）如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止。

     4）每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。

4. 集成学习，Bagging(RF)  Boosting(GBDT Adaboost)

   * bagging: 对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是“三个臭皮匠顶个诸葛亮”的道理
   * boosting: 通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足，通过集成（ensemble）多个弱学习器，通常是决策树，来构建最终的预测模型
   * AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着**梯度最陡**的方向降低损失的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对**异常点**（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性
   * GBDT VS XGBoost：
     * 和GBDT不同，xgboost给损失函数增加了正则化项（L1或L2正则化项，视学习目标不同而取不同正则化参数。
     * 有些损失函数难以计算导数，鉴于这种情况，xgboost使用损失函数的二阶泰勒展开作为损失函数的拟合
     * GBDT是遍历所有特征进行所有可能的划分，再选取最优的。xgboost使用分位点及分位数法，近似的计算，有效降低计算量

5. SVM

   * 拉格朗日乘子

   * KTT

   * SMO

   * 松弛变量

   * 核函数

     * 1. Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想

       2. RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交叉验证来寻找合适的参数，不过这个过程比较耗时。我个人的体会是：使用libsvm，默认参数，RBF核比Linear核效果稍差。通过进行大量参数的尝试，一般能找到比linear核更好的效果。

       3. 1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM

          2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel

          3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况

       至于到底该采用哪种核，要根据具体问题，有的数据是线性可分的，有的不可分，需要多尝试不同核不同参数。如果特征的提取的好，包含的信息量足够大，很多问题都是线性可分的。当然，如果有足够的时间去寻找RBF核参数，应该能达到更好的效果

   * 最小二乘法

6. 贝叶斯

7. k-近邻  **（懒惰学习，是监督学习，虽然没有显式的训练过程）**

   * KNN：找到一个点周围的最近的一群点，靠这一群点投票预测结果，可以分类，投票取高，可以回归，取平均
   * KD树

8. **聚类**

   * 聚类指标
     * 外部指标（与参考模型的结果比较）：Jaccard系数  FM指数 Rand指数
     * 内部指标：DB指数  Dunn指数

   * 距离：

     * 闵科夫斯距离（有序属性）： p=1 曼哈顿距离；p =2 欧式距离；

     * VDM距离（无序属性）：第i个样本簇内属性a与b之间的距离表示，

       |某一个簇属性u上取值a的个数占总取值a个个数-某一个簇属性u上取值b的个数占总取值b个数|

     * 混合属性，将上述两者结合

     > 连续属性也叫数值属性，离散属性也叫列明属性，曼哈顿距离也叫街区距离

   * K均值，仅在凸型簇结构上效果最好
   * *DBSCAN*（密度聚类，样本分布的紧密程度）： 根据给定的邻域参数找到所有的核心对象，然后随机选择一个核心对象，找出其密度可达的所有样本点形成一个簇。然后把这个簇里面如果再核心对象中出现过的化将其从核心对象中删掉，继续随机选取一个核心对象，找密度可达的点，删掉出现过的核心对象，直到核心对象为空。
   * AGNES（层次聚类）：自底向上，先合并一个样本的距离最近的两个，然后将多个根据距离再进行合并，直到没有合并的了，树状图

9. 降维， PCA

10. 条件随机场

11. 隐马尔可夫模型

12. EM算法， GMM

13. 感知机

14. 神经网络，前向反向

    