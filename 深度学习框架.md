# 框架 -Note

>1. 如果用户需要在网络训练过程中，一个相同的网络结构需要搭配多套参数，在caffe下面应该怎么实现？
>2. 在反传的过程中，需要对某些变量的梯度按照某些需求做出改变，在caffe下面应该怎么实现？
>3. 需要自己实现一个简单的激活函数，在caffe下面应该怎么实现？

### tensorflow框架

机制是所有的数据在graph中得以呈示与检索，在编程建立graph的过程中，只需关注网络的前向传播过程，而网络反传的详细过程并不需要手动地去刻画

* 训练和测试数据集：用txt文档记录所有训练与测试图片的索引，作用室给数据传送接口调用

* 数据传送接口：

  * 使用的tensorflow的官方接口去定制的数据读取，首先使用string_input_producer将输入文件名转化成string类型的queue，然后，数据内容在最后tf.Session().run的时候会按照需求出队

    在数据接口中，tensorflow已经在做一些对用户透明的处理了，比如说在底层新建队列，并且通过一些多线程的机制在队列滚动的时候送出batch大小的数据，通过几行代码就可以轻松实现，并且能够非常简便地设置batch的规模。可是，有大量的操作对用户来说是**完全透明**的，我们并不知道队列在怎么处理，并且，tensorflow提供的数据处理接口是相当有限的。

  * 先用placeholder把tensor的位置占住，然后在程序中正常安排tensor的处理逻辑，而最后需要给tensor送值的时候，通过自己定制的ImageReader送值给tensor。可以非常自由地定制程序架构，尤其在数据读取方面。在自定制的ImageReader中，不仅可以自由地安排数据读取库，又可以随意地在数据接口中做一些preprocess，并且该过程对于用户而言不是透明的，用户能够把握数据送入的绝大部分过程。

    在进行ImageReader定制的过程中，笔者往往结合train_data.txt，从外存中读取数据，再送入网络进行迭代，用户可根据自己的需求进行各式各样的定制

* 网络定义文件

  * 网络底层代码与网络高层代码相互分离，网络高层代码调用网络底层代码，在训练主控代码中，程序只调用网络高层代码

  * 在构造训练代码中，tf.get_variable(name, )和with tf.variable_scope(name)中的name是相当重要的。因为在tensorflow中，**参数名称是按照类似堆栈的架构一级一级由底往上堆叠的**，而每一次添加scope中的name就组成了堆栈的一部分。因此，如果说要满足同一个网络有两套不同的参数，又不需重新定义网络结构，应该怎么做呢？就将参数名称堆栈的栈顶换掉就行了，而栈顶以下的部分是不需要改动的。在网络定义文件中，网络高层代码和网络底层代码共同完成了对参数名称堆栈的除栈顶以外部分的定义与约束，而参数的栈顶部分则可以在训练主控程序中定制，这样就可以实现同一网络结构配置多套参数。并且，代码更加层次分明！

    

* 训练主控文件

  *  对于训练的主控程序，首先往往需要在其中定义训练参数，这样方便在代码中修改。然后，可能需要一些输入占位符定义，然后最主要的是进行网络前传得到训练结果，并根据这个结果计算loss，计算loss之后，需要使用训练器对网络进行反传并更新参数梯度。最后，根据需要导入fine-tune的参数，进行初始化，然后就是不停地送入数据并且进行网络的训练。

    

* 测试主控文件

  * 测试程序并不需要计算loss和设置训练器进行网络参数的训练

* 辅助文件