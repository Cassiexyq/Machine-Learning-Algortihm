* **1. 为什么要引入非线性？**

  第一，对于神经网络来说，网络的每一层相当于f(wx+b) = f(w'x)，对于线性函数，其实相当于f(x)=x，一直都是线性的，不管网络多深，也只是反复的取乘以输入，这就像是感知机

  第二，非线性变换是深度学习有效的原因之一，原来线性不可解的问题变得可解，能够取拟合复杂系统

* **2. 为什么relu要比tanh sigmod好？**

  * 采用sigmod 计算量大，算激活函数有指数运算，反向传播也有涉及除法和指数运算，用relu计算量会减少很多

  * sigmod反向传播，容易出现梯度消失情况，当sigomd接近饱和区，变化太缓慢，导数趋于0，容易造成信息丢失，从而无法完成训练，而relu没有饱和的倾向，不会有特别小的梯度出现

  * relu会使一部分神经元的输出为0，在成网络的稀疏性，减少参数的相互依存关系，缓解了过拟合

* **relu的变体**

  leaky relu :在输入小于0的时候不让输出0，而是乘以一个较小的系数，从而保证有导数，同样目的的还有Erelu，还有一个maxout，使用两套w,b参数，输出较大值，本质上可以看成是relu的泛化版本，如果w,b有一套全是0， 就是普通的relu，maxout可以客服relu的缺点，但是参数量翻倍

* **区别**

  * sigmod【0，1】不是关于原点对称，计算麻烦，会有梯度弥散
  * tanh【-1，1】梯度弥散，解决了原点对称问题，比sigmod快
  * relu 【max(0,x)】梯度弥散问题没有完全解决，在左边相当于神经元死亡

* **简单说一下 sigmod**

  常用的非线性激活函数有sigmod， tanh, relu，前两者比较常见于全连接层，后者relu常见于卷积层

  $$g(z) = \frac{1}{1 + e^{-z}}$$

  z是一个线性组合，比如等于 b+ w1\*x1+w2\*x2，通过带入很大的整数或很小的复数到g(z)可知，其结果趋近于0或1，用处是以概率的形式输出，输出为0.9可以解释为90%的概率为正样本

* **3. 为什么LSTM有两个激活函数，而不是统一一个？**

* **4. 如何解决RNN梯度爆炸问题和梯度弥散问题？**

* **5. 简答介绍一下tensorflow的计算图？**

  是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图当是一种有向图，每个节点都是计算图上的一个tensor，也就是张量，而节点之间描述了计算之间的依赖关系和数学操作

* **6. 有哪些调参经验**？

  * 参数初始化，he（relu）  Xavier(适用于tanh,sigmod)， 每个初始化方法的方差
  * 数据预处理，pca白化，0-均值
  * 训练技巧，梯度归一化，算出来的梯度要除以每个批次的size，可以限制最大梯度，设置一个阈值，dropout对小数据防止过拟合的效果比较好，adam在小数据上不如sgd，用sgd先选择0.1，1的学习率， 然后看验证集，如果cost没有将就对半，也可以先用ada系列先跑，快收敛的时候环sgd
  * 尽量做shuffle，利用BN
  * 集成 ensemble，同样的参数，不同的初始方式；不同的参数，交叉验证选区最好的机组；同样的参数，不同迭代次数的模型；不同的模型，进行线性融合

* **7.梯度消失和梯度膨胀**

  梯度消失，根据链式法则，如果每层神经元对上一层的输出的偏导乘以权重结果小于1 ，即使结果0.99，经过多层传播，偏导会趋向0，

  梯度膨胀，结果大于1 ，最后趋向无穷，

  确认出现梯度爆炸：模型无法训练数据中获得更新，低损失；模型不稳定，导致损失出现显著变化；训练过程损失编程NaN       

  方法：激活，梯度截断，权重正则化，使用更小的batch对网络寻览有好处

  都可以使用激活函数，也可以使用BN

* **8. 简单说一下CNN常用模型**
  * AlexNet， 引入Relu和dropout，引入数据增强，池化，5个卷积和3个最大池化和三个全连接层
  * VGG，采用1X1和3x3的卷积核和2x2的最大池化是深度变得更深
  * Google, 在控制计算量和参数量的同时，提高了分类性能
    * 去除最后的全连接层，用一个全局的平均池化取代
    * 引入inception module，是一个4个分支的结构，所有分支都用到了1X1卷积，是因为1X1卷积可以用更少的参数达到非线性和特征变换
    * Inception V2将所有5X5编程2个3X3，提出BN
    * Inception V3把较大的二维卷积拆成了两个较小的一维卷积，加速运算，减少过拟合，更改了inception module结构
  * ResNet，引入shortcut，让深度变得更深，第二个版本还将relu激活函数 编程了y=x的线性函数

* **9. 简单说一下 rcnn fastrcnn fasterrcnn**
  * rcnn用ss来获得可能是object的若干图像局部区域，然后把这些区域分别输入CNN，得到区域的feature，再在feture上加上分类器，判断feature对应的的区域属于具体某一类还是背景，其中还对区域对应的feature做了针对boundingbox的回归，用来修正位置。存在重复计算的问题，region proposal有几千个，大多都是互相重复的，重叠部分就会重复提取
  *  借助SPPnet思想，有了fast rcnn，跟rcnn最大的区别是将proposal的regoin隐射到CNN  的最后一层卷积上，这样一张图片只需要提取一次feature，大大提高了速度，但它的瓶颈在region proposal上 
  * 于是把候选框的提取也交给了CNN来做，提出了faster rcnn，输入的region proposal是固定的，把一张图划分成nxn个区域，每个区域给出9个不同的ratio和scale的proposal，输出是对输入的proposal属于背景还是前景的判断和对其位置的修正，速度更快了
  * RCNN系列框架制约精度提升的瓶颈是将detection问题转化成了对图片局部区域的分类问题，不能充分利用图片局部object在整个图片种的语义信息                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                

* **10. 哪些方法防止过拟合**

  * dropout , l1l2正则化，BN， 网络bagging，早停，调整网络结构。减少数据特征数

* **11.CNN有哪些关键层**

  输入层，对数据取均值，做增强；卷积层，提取 feature；激活层，非线性变化；池化层，下采样；全连接层，增加模型非线性；BN层，缓解梯度弥散

* **12. gru lstm的区别**

* **13. 模型效果不好们可以做哪些处理**

  * 选择合适的损失函数，平方损失，交叉熵损失，神经网络的而损失函数都是非凸的，有多个局部最有点，目标是找到一个可用的最低点
  * 选择合适的批次size，使用批次方法学习，一方面可以减少计算量，另一方面可以帮助走出局部最优点，batch的选择很重要，选的太大会陷入局部最小值，太小会抖动的很厉害，一般64
  * 选择合适的激活函数，把卷积层的而输出做非线性映射，sigmod是一个平滑函数，具有连续性和可微性，最大优点就是非线性，但容易梯度弥散；relu可缓解
  * 选择合适的自适应学习率，学习率太大会抖动，太小训练慢

* **14. 是不是数据越多更有利于神经网络，是不是层数越多网络越好**

* **15.不同层的权重是否已不同的速度收敛**

  是

* **16. finetune的选择**

  * 新的数据集小且和原数据相似： 因为数据集小，如果fine-tune可能过拟合，又因为相似，高层特征类似，使用预训练网络当作特征提取器，训练分类器即可

  * 新的数据集大和原数据相似： 可以finetune整个网络

  * 新的数据集小，原数据不相似：小最好不要finetune，不类似最好不要使用高层特征，可以使用前面层的而特征来训练分类器

  * 新的数据集大，不相似，可以重新训练，或者finetune整个网络

    > finetune要使用更小的学习率，数据集小容易过拟合，不类似不使用高层特征

* **17.为什么有边框回归**

  飞机的例子，即便红色的框能识别出是飞机，但由于红色的框定位不准确，iou<0.5，相当于没有正确的检测出飞机，所以用边框回归微调窗口

  对于窗口一般使用四维向量（x,y,w,h）分别表示中心点坐标和宽高，目标是寻找一种关系使得输入原始的窗口p经过映射得到一个跟真实窗口更为接近的回归窗口

  要如何变换才能从变道接近于真实窗口：   L1损失

  rcnn的做法：

  * 先做平移（x,y），再做尺度缩放(w,h)，边框回归学习就是学习这四个变量
  * 设计算法得到这四个变量
    * 线性回归就是给定输入的特征向量x，学习一组参数w，使得线性回归后的值和真实值非常接近，在边框回归中的输入和输出：
      * 输入：真正的输入是这个窗口对应的CNN的特征，也就是pool5的feature
      * 输出：有一个回归层获得四个变换值，平移量（tx,ty）和尺度缩放（tw,th）。根据变换值在预测值进行计算得到微调后的结果

* **18. SS的思想**

  * 使用一种过分割手段，将图像分割乘小区域（1k-2k）
  * 查看所有小区域，按照合并规则合并可能性最高的相邻两个区域，重复知道整张图像合并乘一个区域位置
  * 输出所有曾经存在过的区域，即为候选区域
  * 优先合并：颜色相近的，纹理相近的，合并后面积小的，合并后，总面积在bbox中所占比例大，保证合并后 形状规则

* **19. anchor的概念**

  当使用3x3的卷积在最后一个feature map上滑动，当滑动到特征图的某个位置时，以当前滑动窗口中心为中心映射回原图的一个区域，（此时的一个点是可以映射回原图某一个区域，相当于感受野），以原图这个区域的中心对应一个尺度和长宽比，就是一个anchor， faster使用3个尺度和3个长宽比，则每个滑动窗口位置就有9个anchor，如果最后一个feature map长宽是38X50，那么原图上可以获得 38x50x1x9个anchor

* **20. CNN的特点和优势**

  局部连接，可以提取局部特征

  权值共享，减少参数数量，可以全部共享，也可以局部共享（人脸，眼睛鼻子嘴由于位置和样式相对固定，可以用链不一样的卷积核）

  降维，池化或stride

  多层次结构，将低层次局部特征组合乘较稿层次的特征，不同层次特征对应不同的任务

* **21. BN层**

  为什么数据需要归一化，归一化有什么好处

  * 神经网络的学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低，需要使用输入归一化方法，使得训练数据与测试数据的分布相同

  * 如果神经网络在训练过程中的某一层的输入数据分布发生了变化，这一层就需要去适应这个新的数据分布，影响训练速度，为了让训练更加高效，提出了SGD，但需要认为选择参数，学习率，参数初始化，权重衰减系数，dropout率等，另一个问题是无论怎么训练，可能导致梯度始终变化很小，可能到了激活函数的饱和区状态，加入正则的一个原因就是为了让输入值能经过激励函数的敏感部位，这个不敏感问题不仅仅发生在输入层，也可能再隐藏层

  BN 的本质原理： 在输入前先做归一化，均值0，方差1，再进入下一层，但这个归一化是一个可学习，有参数的网络层

  如何训练BN的参数就是链式法则-具体推导见草稿

  BN保证了每一层输入的稳定，可以加速训练，帮助减少梯度消失核梯度爆炸问题

* **22 . 逻辑回归为什么要对特征进行离散化**
  * 离散特征的增加、减少很容易，易于模型迭代
  * 稀疏向量内积运算速度快
  * 离散化的而特征对一场值有很强的鲁棒性，比如年龄 》 30为1否则0，如果特征没有离散化，年龄》300会造成很大困扰
  * 属于广义线性模型，表达能力受限，每个变量有单独的权重，相当于引入非线性，提升表达能力
  * 离散化可以特征交叉，有m+n个编程m*n个
  * 特征离散化降低模型过拟合

* **23. 逻辑回归**

​         给定一些数据，用y表示类别，一个线性分类器的学习目标是在n维数据空间中找到 一个超平面，这个超平面可以表示 $$ w^T x+b = 0$$

- [ ] >   LR 工程化应用，并行处理等

* **24. 正则化**

​	l2正则： 目标函数中增加所有权重w参数的平方之和，逼迫所有w尽可能趋向0但不为0，因为过拟合的时候，拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的而变化剧烈，儿某些w非常大，l2惩罚了权重变大的趋势

​	l1正则： 目标函数中增加所有权重w参数的绝对值之和，逼迫w为0， 也就是变稀疏，实际上x中的大部分元素也就是特征都是核最终的Y没有关系或者不提供任何 信息的，在最小化目标函数时考虑这些额外的特征，虽然可以获得更小的误差，但在预测新样本时，这些没用的特征也会被考虑进行，会干扰正确的预测

 L1 先验服从拉普拉斯分布， L2 服从高斯分布

* **25. SVM和LR的联系和区别**

  联系

  * 都能处理分类问题，且一般处理线性二分类问题
  * 都可以增加正则化项

  区别

  * LR是参数模型，SVM是非参数模型
  * 从目标函数看，逻辑回归采用的是交叉熵损失函数，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少于分类关系较小的数据点的权重
  * SVM 处理方式只考虑 支持向量，也就是和分类最相关的少数点学习分类器，逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重
  * 逻辑回归在大规模线性费雷是比较方便，SVM转为对偶问题只需计算少数几个支持向量的距离，在解决复杂核函数计算优势明显
  * LR能做的SVM都能做，SVM能做的LR不能做

* **26. 牛顿法和梯度下降， 拟牛顿法，共轭梯度 **

  * 从收敛速度上看，牛顿法是二阶收敛，梯度下降是一阶收敛，前者更快，牛顿法虽然是局部算法，只是在局部上看到更细致，梯度法仅考虑方向，牛顿法不但考虑方向，还考虑步子的而大小

  * 牛顿法是一种迭代算法，每一步都要计算目标函数的海森矩阵的逆矩阵，计算复杂

  * 拟牛顿法：DFP BFGS，不需要二阶导数，使用正定矩阵来近似海森矩阵的逆，只要求和最速下降法一样的每一步迭代时目标函数的梯度

    为什么不用牛顿法作为梯度下降的算法：

    * 牛顿法要求计算目标函数的二阶导数，在高维下矩阵巨大，计算和存储都成问题
    * 使用小批量，牛顿法对于二阶导数的估计噪音很大
    * 在目标函数非凸时，牛顿法容易受到鞍点甚至最大值点的吸引

  * 共轭梯度介于最速下降法和牛顿法之间的一个方法，仅需利用一阶导数，优点存储量小，桌布收敛，稳定，不需要任何外来参数

* **27. 相对熵，条件熵，互信息**

  相对熵： 又称交叉熵，KL散度

* **28. kmeans**

  时间复杂度： O(tkmn)  t迭代次数 k簇的数目  m 记录数  n 维数

  空间复杂度 ： O((m+k)n)   

* **29.xgboost如何寻找最优特征，又放回还是无放回**

  xgboost在训练过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据，从而记忆了每个特征在对模型训练时的重要性，从根到叶子中间节点涉及某特征的次数作为该特征重要性的排序

  样本不放回，时boosting算法，每轮计算样本不重复，支持子采样，也就是每一轮计算不使用全部样本减少过拟合，还有列采样，每轮按照百分比随机采样一部分特征，提尕计算速度防止过拟合

* **30. 线性分类器和非线性分类器**

  线性：LR, 贝叶斯，单层感知机，线性回归

  非线性： 决策树， RF, GBDT， 多层感知机

* **31.贝叶斯分类器**

  基于先验概率推导出后验概率，有标签，可以用计大似然估计法解贝叶斯分类器

  结构风险最大化就是贝叶斯估计中的最大后验概率

  用极大似然估计可能会出现所要估计的概率值为0，所以采用贝叶斯估计，再分母加上一个值，这个数较拉普拉斯平滑

* **32. 隐马尔科夫**

  从隐藏的马尔可夫链中随机生成一个不可观测的状态序列，对每个状态随机生成一个观测而产生观测序列的过程

  概率计算--前向后向； 学习算法--EM算法，Baum-Welch； 预测， 维特比算法，从观测到状态

  EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法

  维特比算法： 用动态规划解决HMM的预测问题，不是参数估计

  前向后向：用来算概率

  极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数

* **33. RCNN 的 hard negative mining** - 难分样本问题

  Pos  iou>0.5

  neg [0.1,0.5]

  hard neg [0,0.1]  把这部分重新放入进行分类训练

  难负例挖掘（Hard Negative Mining）就是在训练时，尽量多挖掘些难负例（hard negative)加入负样本集，这样会比easy negative组成的负样本集效果更好

  而R-CNN中的难负例挖掘就是采用了这种自举法（bootstrap）的方法：

  - - 先用初始的正负样本训练分类器（此时为了平衡数据，使用的负样本也只是所有负样本的子集）
    - 用（1）训练好的分类器对样本进行分类,把其中错误分类的那些样本(hard negative)放入负样本子集，
    - 再继续训练分类器,
    - 如此反复,直到达到停止条件(比如分类器性能不再提升).

  > **也就是说，R-CNN的Hard Negative Mining相当于给模型定制一个错题集，在每轮训练中不断“记错题”，并把错题集加入到下一轮训练中，直到网络效果不能上升为止。**

  OHEM : online  hard negnative mining

  　　1、focal loss：通过模型预测的概率pt，使用(1-Pt)来代表样本难分程度。可以理解为模型对某个样本预测属于其真实label的概率越高，则说明该样本对此模型比较容易学习，反之则难分。 
  　　2、《ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks》论文提出一个附加网络来帮助主网络区分样本难易程度。 
  　　3、《Fine-tuning Convolutional Neural Networks for Biomedical Image Analysis》论文通过对一张图像进行数据增强生成多张图像，然后使用模型预测每张图像的概率。根据多张相同label的增强图像的概率分布区分其样本难易程度。 
  　　4、《OHEM: Training Region-based Object Detectors with Online Hard Example Mining》论文提出先使用模型输出概率，据此选出部分难分样本，然后根据这些样本，更新网络参数。

  OHEM（online hard example miniing）算法的核心思想是根据输入样本的**损失**进行筛选，筛选出hard example，表示对分类和检测影响较大的样本，然后将筛选得到的这些样本应用在随机梯度下降中训练。在实际操作中是将原来的一个ROI Network扩充为两个ROI Network，这两个ROI Network共享参数。其中前面一个ROI Network只有前向操作，**主要用于计算损失**；后面一个ROI Network包括前向和后向操作，以hard example作为输入，计算损失并回传梯度。作者将该算法应用在Fast RCNN中，网络结构还是采用VGG16和VGG_CNN_M_1024，数据集主要采用VOC2007，VOC2012和COCO数据集。

  > **OHEM:ROI经过ROI plooling层生成feature map，然后进入只读的ROI network得到所有ROI的loss；然后是hard ROI sampler结构根据损失排序选出hard example，并把这些hard example作为下面前向后向的ROI network的输入。**

* **34. ROI Pooling 以及 改进的方法**

  roi-Align（17） ： 解决了两次量化问题，但引入n参数，不是所有点参与运算，只有4个点进行双线性插值得到一个点，具体就是当我们从原图映射到featuremap上要除以16，那先保留浮点数，得到这样的框，然后对这个框进行 n xn的划分，对每个n里面获得中点，这个中点时根据4个点的双线性插值得到，既而这个n是新生的参数问题

  precise roi pooling(Iou-net, 18)进行改进，让所有的点参与运算，保留浮点数得到框的位置后，产生虚拟的在框上的点（这些点也是通过双线性插值得到的），这个生成的点的值是在这个点周围原来的4个点的avg pooling，最后对这些虚拟点进行打格计算

* **35. 贝叶斯分类**

  对于朴素贝叶斯分类器的操作可以理解如下：

  <https://blog.csdn.net/leviopku/article/details/80145178>

  对离散数据和连续数据的处理

  需要拉普拉斯进行修正：某个属性值在训练集中没有与某个类同时出现过。

* **36. 为什么one-stage的速度快，two-stage的精度高**

  one-stage受制于类别不平衡，相比twostage更严重

  * 什么是“类别不平衡”呢？

    详细来说，检测算法在早期会生成一大波的bbox。而一幅常规的图片中，顶多就那么几个object。这意味着，绝大多数的bbox属于background。

  * “类别不平衡”又如何会导致检测精度低呢？

    因为bbox数量爆炸。 
    正是因为bbox中属于background的bbox太多了，所以如果分类器无脑地把所有bbox统一归类为background，accuracy也可以刷得很高。于是乎，分类器的训练就失败了。分类器训练失败，检测精度自然就低了。

  * 那为什么two-stage系就可以避免这个问题呢？

    因为two-stage系有RPN罩着。 
    第一个stage的RPN会对anchor进行简单的二分类（只是简单地区分是前景还是背景，并不区别究竟属于哪个细类）。经过该轮初筛，属于background的bbox被大幅砍削。虽然其数量依然远大于前景类bbox，但是至少数量差距已经不像最初生成的anchor那样夸张了。就等于是 从 “类别 极 不平衡” 变成了 “类别 较 不平衡” 。 
    不过，其实two-stage系的detector也不能完全避免这个问题，只能说是在很大程度上减轻了“类别不平衡”对检测精度所造成的影响。 
    接着到了第二个stage时，分类器登场，在初筛过后的bbox上进行难度小得多的第二波分类(这次是细分类)。这样一来，分类器得到了较好的训练，最终的检测精度自然就高啦。但是经过这么两个stage一倒腾，操作复杂，检测速度就被严重拖慢了。

  * 那为什么one-stage系无法避免该问题呢？

    因为one stage系的detector直接在首波生成的“类别极不平衡”的bbox中就进行难度极大的细分类，意图直接输出bbox和标签（分类结果）。而原有交叉熵损失(CE)作为分类任务的损失函数，无法抗衡“类别极不平衡”，容易导致分类器训练失败。因此，one-stage detector虽然保住了检测速度，却丧失了检测精度。
    

* **37. 局部最优和全局最优**

  （1）局部最优，就是在函数值空间的一个**有限区域**内寻找最小值；而全局最优，是在函数值空间**整个区域**寻找最小值问题。

  （2）函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。

  （3）全局最小点是那种它的函数值小于或等于所有的可行点。

* **38. 机器学习的学习方式**

  * 监督：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。

  * 非监督：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构，Apriori算法以及k-Means算法。
  * 半监督学习：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）
  * 弱监督学习：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。

* **39. 逻辑回归的适用性**

  * 用于概率预测，用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。

  * 用于分类，实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类
  * 仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。
  * 各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。

* **40. 为什么需要代价函数，为什么非负**

  * 为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数得到参数
  * 用于找到最优解的目的函数

  目标函数存在下界，在优化过程中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就证明能够收敛，只要设计的目标函数有下界，基本上都可以，非负更为方便

* **41. bias variance**

  偏差：衡量模型拟合数据的能力，反映的是在样本上的输出和真实值之间的误差，偏差越大，越偏离真实数据

  方差：描述的是预测值的变化范围，反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。

  如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。

* **42. 解决过拟合和欠拟合问题**

  欠拟合：添加特征，组合，泛化，相关，上下文特征；添加多项式特征‘增加模型复杂程度；减小正则化系数

  过拟合： 重新清洗样本，可能数据不纯；增加样本；降低模型复杂程度；增大正则化系数，dropout；提早停止；增大学习率；添加噪声；对树剪枝；减少特征

  