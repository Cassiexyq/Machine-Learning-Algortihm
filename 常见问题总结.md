* **1. 为什么要引入非线性？**

  第一，对于神经网络来说，网络的每一层相当于f(wx+b) = f(w'x)，对于线性函数，其实相当于f(x)=x，一直都是线性的，不管网络多深，也只是反复的取乘以输入，这就像是感知机

  第二，非线性变换是深度学习有效的原因之一，原来线性不可解的问题变得可解，能够取拟合复杂系统

* **2. 为什么relu要比tanh sigmod好？**

  * 采用sigmod 计算量大，算激活函数有指数运算，反向传播也有涉及除法和指数运算，用relu计算量会减少很多

  * sigmod反向传播，容易出现梯度消失情况，当sigomd接近饱和区，变化太缓慢，导数趋于0，容易造成信息丢失，从而无法完成训练，而relu没有饱和的倾向，不会有特别小的梯度出现

  * relu会使一部分神经元的输出为0，在成网络的稀疏性，减少参数的相互依存关系，缓解了过拟合

* **relu的变体**

  leaky relu :在输入小于0的时候不让输出0，而是乘以一个较小的系数，从而保证有导数，同样目的的还有Erelu，还有一个maxout，使用两套w,b参数，输出较大值，本质上可以看成是relu的泛化版本，如果w,b有一套全是0， 就是普通的relu，maxout可以客服relu的缺点，但是参数量翻倍

* **区别**

  * sigmod不是关于原点对称，计算麻烦，会有梯度弥散

  * tanh梯度弥散，解决了原点对称问题，比sigmod快
  * relu 梯度弥散问题没有完全解决，在左边相当于神经元死亡

* **3. 为什么LSTM有两个激活函数，而不是统一一个？**

* **4. 如何解决RNN梯度爆炸问题和梯度弥散问题？**

* **5. 简答介绍一下tensorflow的计算图？**

  是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图当是一种有向图，每个节点都是计算图上的一个tensor，也就是张量，而节点之间描述了计算之间的依赖关系和数学操作

* **6. 有哪些调参经验**？

  * 参数初始化，he（relu）  Xavier(适用于tanh,sigmod)， 每个初始化方法的方差
  * 数据预处理，pca白化，0-均值
  * 训练技巧，梯度归一化，算出来的梯度要除以每个批次的size，可以限制最大梯度，设置一个阈值，dropout对小数据防止过拟合的效果比较好，adam在小数据上不如sgd，用sgd先选择0.1，1的学习率， 然后看验证集，如果cost没有将就对半，也可以先用ada系列先跑，快收敛的时候环sgd
  * 尽量做shuffle，利用BN
  * 集成 ensemble，同样的参数，不同的初始方式；不同的参数，交叉验证选区最好的机组；同样的参数，不同迭代次数的模型；不同的模型，进行线性融合

* **7.梯度消失和梯度膨胀**

  梯度消失，根据链式法则，如果每层神经元对上一层的输出的偏导乘以权重结果小于1 ，即使结果0.99，经过多层传播，偏导会趋向0，

  梯度膨胀，结果大于1 ，最后趋向无穷，

  确认出现梯度爆炸：模型无法训练数据中获得更新，低损失；模型不稳定，导致损失出现显著变化；训练过程损失编程NaN       激活，梯度截断，权重正则化，使用更小的batch对网络寻览有好处

  都可以使用激活函数，也可以使用BN

* **8. 简单说一下CNN常用模型**
  * AlexNet， 引入Relu和dropout，引入数据增强，池化，三个卷积和一个最大池化和三个全连接层
  * VGG，采用1X1和3x3的卷积核和2x2的最大池化是深度变得更深
  * Google, 在控制计算量和参数量的同时，提高了分类性能
    * 去除最后的全连接层，用一个全局的平均池化取代
    * 引入inception module，是一个4个分支的结构，所有分支都用到了1X1卷积，是因为1X1卷积可以用更少的参数达到非线性和特征变换
    * Inception V2将所有5X5编程2个3X3，提出BN
    * Inception V3把较大的二维卷积拆成了两个较小的一维卷积，加速运算，减少过拟合，更改了inception module结构
  * ResNet，引入shortcut，让深度变得更深，第二个版本还将relu激活函数 编程了y=x的线性函数