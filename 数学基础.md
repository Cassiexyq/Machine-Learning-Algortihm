## 数学基础

#### 1.	线性代数

* 标量 向量  张量： 一个标量就是一个单独的数，一个向量是一列数，把标量当作零阶张量，矢量当作一阶张量，矩阵就是二阶张量
* 范式： L1范式和L2范式，主要在损失函数中起到一个限制模型复杂度的作用
* 特征分解，分解成特征向量和特征值$$A = Vdiag(\lambda)V^{-1}$$
* 奇异值分解（SVD）：矩阵要能够特征分解的前提是能够对角化，但很多不满足，所以有了SVD，可以得到类似于特征分解的信息。$$A = UDV^T$$  UV都是正交矩阵，D是对角矩阵
* 距离公式
* 夹角余弦：衡量两个向量方向的差异，夹角余弦越大，表示两个向量的夹角越小，当两个向量重合为1，相反为-1，机器学习中用这一概念来衡量两个样本之间的差异
* 汉明距离：两个字符串中不相同的位数
* 杰卡德相似系数： 两个集合AB的交集在AB并集所占的比例，衡量两个集合的相似度的一种指标，用来衡量样本的相似度

### 2. 概率

* 条件概率：某个事件在给定其他事件发生时的概率，叫条件概率

* 贝叶斯公式

  $$P(A|B) = \frac{P(AB)}{P(B)}$$

  $$P(B|A) = \frac{P(AB)}{P(A)}$$

  $$P(AB) = P(A|B)P(B)$$

  $$P(AB) = P(B|A)P(A)$$

  $$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

​       全概率公式：$$P(A) = \sum_{i=1}^{N}P(A|B_i)P(B_i)$$

​	带入贝叶斯 $$P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{i=1}^NP(A|B_i)p(B_i)}$$

* 期望：数学期望是试验中每次可能结果的概率乘以其结果的综合，反应随机变量平均值的大小

  > 所以期望就是随机变量的平均值，计算方差就是样本减期望平方的平均值

* 方差：方差用来衡量随机变量与数学期望之间的偏差程度，反映的是模型每一次的输出结果与模型输出期望之间的误差，即模型的稳定性，反映预测的波动情况。统计中的方差为样本方差，是各个样本数据分别与平均值之差的平方和的平均数

  $$V(x) = E([x-E(x)]^2) = E(x^2)-(E(x))^2$$

